{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbcb0b55",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0026c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = ['pandas', 'numpy', 'requests', 'python-dotenv', 'google-generativeai', 'json5']\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "\n",
    "print(\"All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import json5\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Gemini API (you can also use OpenRouter)\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "else:\n",
    "    print(\"Warning: GEMINI_API_KEY not found. Please set it in .env file\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96af923",
   "metadata": {},
   "source": [
    "## Section 2: Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a0888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demonstration, we'll create sample Yelp reviews data\n",
    "# In production, you would download from: https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset\n",
    "\n",
    "sample_reviews = [\n",
    "    {\"review\": \"Absolutely amazing service! The food was fresh and delicious. Highly recommend this place!\", \"actual_stars\": 5},\n",
    "    {\"review\": \"Great atmosphere and friendly staff. Food was good but a bit pricey.\", \"actual_stars\": 4},\n",
    "    {\"review\": \"Average restaurant. Nothing special. Service was slow.\", \"actual_stars\": 3},\n",
    "    {\"review\": \"Pretty bad experience. Cold food and rude staff. Won't come back.\", \"actual_stars\": 2},\n",
    "    {\"review\": \"Terrible! Worst meal I've ever had. Complete waste of money.\", \"actual_stars\": 1},\n",
    "    {\"review\": \"Excellent food quality, innovative menu, wonderful ambiance. Perfect evening!\", \"actual_stars\": 5},\n",
    "    {\"review\": \"Good portions, tasty food. Minor issues with waiting time.\", \"actual_stars\": 4},\n",
    "    {\"review\": \"It was okay. Not worth the hype. Food was mediocre.\", \"actual_stars\": 3},\n",
    "    {\"review\": \"Disappointed. Food arrived cold and overpriced for quality.\", \"actual_stars\": 2},\n",
    "    {\"review\": \"Horrible place. Never again. Service was nonexistent.\", \"actual_stars\": 1},\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(sample_reviews)\n",
    "\n",
    "# For actual testing, sample ~200 rows from your Yelp dataset\n",
    "# df = pd.read_csv('yelp_reviews.csv').sample(200, random_state=42)\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} reviews\")\n",
    "print(f\"\\nSample reviews:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9ca72",
   "metadata": {},
   "source": [
    "## Section 3: Prompt Engineering - Approach 1 (Zero-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f68607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH 1: Zero-Shot Prompting\n",
    "# Simple, direct prompt without examples\n",
    "\n",
    "ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"You are an expert in sentiment analysis. Analyze the following Yelp review and predict the star rating (1-5 stars).\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Return ONLY valid JSON (no markdown, no extra text):\n",
    "{{\n",
    "  \"predicted_stars\": <integer 1-5>,\n",
    "  \"explanation\": \"<brief reasoning>\"\n",
    "}}\"\"\"\n",
    "\n",
    "def call_llm(prompt: str) -> str:\n",
    "    \"\"\"Call Gemini API with the prompt\"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling API: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_json_from_response(response_text: str) -> Dict:\n",
    "    \"\"\"Extract and parse JSON from LLM response\"\"\"\n",
    "    try:\n",
    "        # Try standard JSON parsing\n",
    "        return json.loads(response_text)\n",
    "    except:\n",
    "        try:\n",
    "            # Try json5 for more lenient parsing\n",
    "            return json5.loads(response_text)\n",
    "        except:\n",
    "            # Try to extract JSON from markdown code blocks\n",
    "            if '```json' in response_text:\n",
    "                json_str = response_text.split('```json')[1].split('```')[0].strip()\n",
    "                return json.loads(json_str)\n",
    "            elif '{' in response_text and '}' in response_text:\n",
    "                json_str = response_text[response_text.index('{'):response_text.rindex('}')+1]\n",
    "                return json5.loads(json_str)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "def evaluate_approach(df_sample: pd.DataFrame, prompt_template: str, approach_name: str) -> Dict:\n",
    "    \"\"\"Evaluate a prompting approach on sample data\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating {approach_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for idx, row in df_sample.iterrows():\n",
    "        prompt = prompt_template.format(review=row['review'])\n",
    "        response = call_llm(prompt)\n",
    "        parsed = extract_json_from_response(response)\n",
    "        \n",
    "        result = {\n",
    "            'review_idx': idx,\n",
    "            'actual_stars': row['actual_stars'],\n",
    "            'predicted_stars': parsed.get('predicted_stars', None) if parsed else None,\n",
    "            'explanation': parsed.get('explanation', '') if parsed else '',\n",
    "            'json_valid': parsed is not None and 'predicted_stars' in (parsed or {}),\n",
    "            'raw_response': response[:100] + '...' if len(response) > 100 else response\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(f\"Review {idx}: Actual={row['actual_stars']} | Predicted={result['predicted_stars']} | Valid JSON: {result['json_valid']}\")\n",
    "        time.sleep(1)  # Rate limiting for API\n",
    "    \n",
    "    # Calculate metrics\n",
    "    valid_jsons = sum(1 for r in results if r['json_valid'])\n",
    "    correct_predictions = sum(1 for r in results if r['json_valid'] and r['predicted_stars'] == r['actual_stars'])\n",
    "    \n",
    "    metrics = {\n",
    "        'approach': approach_name,\n",
    "        'total_reviews': len(results),\n",
    "        'valid_json_count': valid_jsons,\n",
    "        'json_validity_rate': valid_jsons / len(results) * 100 if results else 0,\n",
    "        'correct_predictions': correct_predictions,\n",
    "        'accuracy': correct_predictions / valid_jsons * 100 if valid_jsons > 0 else 0,\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Test on first 3 reviews for quick validation\n",
    "test_df = df.head(3)\n",
    "approach1_metrics = evaluate_approach(test_df, ZERO_SHOT_PROMPT_TEMPLATE, \"Approach 1: Zero-Shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88c5416",
   "metadata": {},
   "source": [
    "## Section 4: Prompt Engineering - Approach 2 (Few-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825b3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH 2: Few-Shot Prompting\n",
    "# Includes 3 labeled examples to guide the LLM\n",
    "\n",
    "FEW_SHOT_PROMPT_TEMPLATE = \"\"\"You are an expert in sentiment analysis. Analyze reviews and predict star ratings (1-5 stars).\n",
    "\n",
    "Examples:\n",
    "1. Review: \"Amazing service and delicious food! Highly recommend!\"\n",
    "   JSON: {{\"predicted_stars\": 5, \"explanation\": \"Very positive sentiment with explicit recommendation.\"}}\n",
    "\n",
    "2. Review: \"Good food but slow service. Decent value.\"\n",
    "   JSON: {{\"predicted_stars\": 4, \"explanation\": \"Positive overall with minor complaints.\"}}\n",
    "\n",
    "3. Review: \"Terrible experience. Cold food and rude staff.\"\n",
    "   JSON: {{\"predicted_stars\": 1, \"explanation\": \"Strongly negative sentiment about multiple aspects.\"}}\n",
    "\n",
    "Now analyze this review:\n",
    "Review: {review}\n",
    "\n",
    "Return ONLY valid JSON (no markdown, no extra text):\n",
    "{{\n",
    "  \"predicted_stars\": <integer 1-5>,\n",
    "  \"explanation\": \"<brief reasoning>\"\n",
    "}}\"\"\"\n",
    "\n",
    "approach2_metrics = evaluate_approach(test_df, FEW_SHOT_PROMPT_TEMPLATE, \"Approach 2: Few-Shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede640a",
   "metadata": {},
   "source": [
    "## Section 5: Prompt Engineering - Approach 3 (Chain-of-Thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46337070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH 3: Chain-of-Thought Prompting\n",
    "# Asks the LLM to reason step-by-step\n",
    "\n",
    "COT_PROMPT_TEMPLATE = \"\"\"You are an expert in sentiment analysis. Analyze the following Yelp review by reasoning step-by-step.\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Step 1: Identify the overall sentiment (positive, negative, neutral)\n",
    "Step 2: Look for specific positive keywords or complaints\n",
    "Step 3: Consider the intensity of emotions expressed\n",
    "Step 4: Determine the appropriate star rating (1-5)\n",
    "\n",
    "After your reasoning, return ONLY valid JSON (no markdown, no extra text):\n",
    "{{\n",
    "  \"predicted_stars\": <integer 1-5>,\n",
    "  \"explanation\": \"<brief reasoning based on your analysis>\"\n",
    "}}\"\"\"\n",
    "\n",
    "approach3_metrics = evaluate_approach(test_df, COT_PROMPT_TEMPLATE, \"Approach 3: Chain-of-Thought\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff74fb",
   "metadata": {},
   "source": [
    "## Section 6: Evaluation Metrics and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89247d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Prompting Approach': ['Zero-Shot', 'Few-Shot', 'Chain-of-Thought'],\n",
    "    'Total Reviews': [approach1_metrics['total_reviews'], approach2_metrics['total_reviews'], approach3_metrics['total_reviews']],\n",
    "    'Valid JSON Count': [approach1_metrics['valid_json_count'], approach2_metrics['valid_json_count'], approach3_metrics['valid_json_count']],\n",
    "    'JSON Validity Rate (%)': [f\"{approach1_metrics['json_validity_rate']:.1f}\", f\"{approach2_metrics['json_validity_rate']:.1f}\", f\"{approach3_metrics['json_validity_rate']:.1f}\"],\n",
    "    'Correct Predictions': [approach1_metrics['correct_predictions'], approach2_metrics['correct_predictions'], approach3_metrics['correct_predictions']],\n",
    "    'Accuracy (%)': [f\"{approach1_metrics['accuracy']:.1f}\", f\"{approach2_metrics['accuracy']:.1f}\", f\"{approach3_metrics['accuracy']:.1f}\"]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROMPTING APPROACH COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237af649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis\n",
    "print(\"\\nDETAILED ANALYSIS:\")\n",
    "print(\"\\n1. APPROACH 1 - ZERO-SHOT PROMPTING\")\n",
    "print(f\"   - Strategy: Direct instruction without examples\")\n",
    "print(f\"   - JSON Validity: {approach1_metrics['json_validity_rate']:.1f}%\")\n",
    "print(f\"   - Accuracy: {approach1_metrics['accuracy']:.1f}%\")\n",
    "print(f\"   - Pros: Simple, fast\")\n",
    "print(f\"   - Cons: May produce inconsistent JSON format\")\n",
    "\n",
    "print(\"\\n2. APPROACH 2 - FEW-SHOT PROMPTING\")\n",
    "print(f\"   - Strategy: Provide 3 labeled examples to guide the model\")\n",
    "print(f\"   - JSON Validity: {approach2_metrics['json_validity_rate']:.1f}%\")\n",
    "print(f\"   - Accuracy: {approach2_metrics['accuracy']:.1f}%\")\n",
    "print(f\"   - Pros: Better JSON consistency, examples show expected format\")\n",
    "print(f\"   - Cons: Slightly longer prompt\")\n",
    "\n",
    "print(\"\\n3. APPROACH 3 - CHAIN-OF-THOUGHT PROMPTING\")\n",
    "print(f\"   - Strategy: Ask model to reason step-by-step before predicting\")\n",
    "print(f\"   - JSON Validity: {approach3_metrics['json_validity_rate']:.1f}%\")\n",
    "print(f\"   - Accuracy: {approach3_metrics['accuracy']:.1f}%\")\n",
    "print(f\"   - Pros: Better reasoning, transparent decision-making\")\n",
    "print(f\"   - Cons: More API tokens used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71dff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reliability and consistency check\n",
    "def calculate_consistency(results):\n",
    "    \"\"\"Calculate consistency as the variance in predictions\"\"\"\n",
    "    valid_results = [r for r in results if r['json_valid']]\n",
    "    if not valid_results:\n",
    "        return 0\n",
    "    # All valid predictions match their actual values = high consistency\n",
    "    matches = sum(1 for r in valid_results if r['predicted_stars'] == r['actual_stars'])\n",
    "    return matches / len(valid_results) * 100\n",
    "\n",
    "consistency_data = {\n",
    "    'Approach': ['Zero-Shot', 'Few-Shot', 'Chain-of-Thought'],\n",
    "    'Consistency Score (%)': [\n",
    "        f\"{calculate_consistency(approach1_metrics['results']):.1f}\",\n",
    "        f\"{calculate_consistency(approach2_metrics['results']):.1f}\",\n",
    "        f\"{calculate_consistency(approach3_metrics['results']):.1f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "consistency_df = pd.DataFrame(consistency_data)\n",
    "print(\"\\nCONSISTENCY ANALYSIS:\")\n",
    "print(consistency_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d4a4af",
   "metadata": {},
   "source": [
    "## Section 7: Key Findings and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd436b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS AND RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. JSON VALIDITY RATE:\n",
    "   - Few-Shot and Chain-of-Thought approaches both perform well\n",
    "   - Providing examples (Few-Shot) or explicit reasoning steps improves format compliance\n",
    "   - Zero-Shot may sometimes produce markdown-formatted JSON\n",
    "\n",
    "2. ACCURACY:\n",
    "   - Few-Shot approach typically achieves highest accuracy\n",
    "   - Chain-of-Thought provides better reasoning transparency\n",
    "   - Zero-Shot works but may miss nuances in sentiment\n",
    "\n",
    "3. RELIABILITY & CONSISTENCY:\n",
    "   - Few-Shot: Most reliable due to example guidance\n",
    "   - Chain-of-Thought: Good reliability with transparent reasoning\n",
    "   - Zero-Shot: Lower consistency but faster responses\n",
    "\n",
    "4. RECOMMENDATION:\n",
    "   ✓ For production use: FEW-SHOT prompting\n",
    "     - Best balance of accuracy, JSON validity, and consistency\n",
    "     - Examples guide the model to correct format and reasoning\n",
    "   \n",
    "   ✓ For explainability: CHAIN-OF-THOUGHT prompting\n",
    "     - Transparent reasoning helps understand model decisions\n",
    "     - Better for debugging and gaining user trust\n",
    "   \n",
    "   ✓ For speed/efficiency: ZERO-SHOT prompting\n",
    "     - Faster responses with fewer tokens\n",
    "     - Good for high-volume applications\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8bca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to CSV for documentation\n",
    "results_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Approach': 'Zero-Shot',\n",
    "        'JSON_Validity_Rate': f\"{approach1_metrics['json_validity_rate']:.1f}%\",\n",
    "        'Accuracy': f\"{approach1_metrics['accuracy']:.1f}%\",\n",
    "        'Consistency': f\"{calculate_consistency(approach1_metrics['results']):.1f}%\",\n",
    "        'Total_Reviews_Tested': approach1_metrics['total_reviews']\n",
    "    },\n",
    "    {\n",
    "        'Approach': 'Few-Shot',\n",
    "        'JSON_Validity_Rate': f\"{approach2_metrics['json_validity_rate']:.1f}%\",\n",
    "        'Accuracy': f\"{approach2_metrics['accuracy']:.1f}%\",\n",
    "        'Consistency': f\"{calculate_consistency(approach2_metrics['results']):.1f}%\",\n",
    "        'Total_Reviews_Tested': approach2_metrics['total_reviews']\n",
    ",\n",
    ",\n",
    "{approach3_metrics['json_validity_rate']:.1f}%\","
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
